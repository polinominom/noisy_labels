# -*- coding: utf-8 -*-
"""chexpert_noisfy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Up66s8R2xK4yRqBX0sGYIZpyZ1ncOnhd
"""

import tensorflow as tf
import pandas as pd
import os
import datetime
from google.colab import drive
import pickle
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D , Flatten
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import numpy as np
from tensorflow.keras.callbacks import CSVLogger

drive.mount('/content/gdrive')

"""# Model Implementations

## Densenet
"""

from tensorflow.keras.applications.densenet import DenseNet121
def get_densenet():
  return DenseNet121(include_top=True, 
                      weights=None,
                      input_shape=(256, 256, 3), 
                      classes=2)

"""## Vgg"""

from tensorflow.keras.applications.vgg16 import VGG16
def get_vgg16():
  return VGG16(include_top=True, 
                weights=None,
                input_shape=(256, 256, 3), 
                classes=2)

"""## Resnet"""

from tensorflow.keras.applications.resnet import ResNet50
def get_resnet50():
  return ResNet50(include_top=True, 
                  weights=None,
                  input_shape=(256, 256, 3), 
                  classes=2)

"""## model compile func"""

def compile_model(model, binary=False):
  # Instantiate a logistic loss function that expects integer targets.
  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
  # Instantiate an accuracy metric.
  accuracy = tf.keras.metrics.SparseCategoricalAccuracy()
  if binary:
    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)
    accuracy = tf.keras.metrics.binary_accuracy
  
  # Instantiate an optimizer.
  optimizer = tf.keras.optimizers.Adam()
  # Instantiate some callbacks
  
  # NO EARLY STOPPING FOR NOW
  #callbacks = [tf.keras.callbacks.EarlyStopping(), tf.keras.callbacks.ModelCheckpoint(filepath='my_model.keras', save_best_only=True)]
  callbacks = [tf.keras.callbacks.EarlyStopping(), tf.keras.callbacks.ModelCheckpoint(filepath='my_model.keras', save_best_only=False), CSVLogger("model_history_log.csv", append=True)]
  
  model.compile(optimizer=optimizer, loss=loss,metrics=[accuracy])
  return model

m =get_densenet()
m.summary()

m2 = get_resnet50()
m2.summary()

m3 = get_vgg16()
m3.summary()

# compile model example
m = compile_model(m, binary=True)
m2 = compile_model(m2, binary=True)
m3 = compile_model(m3, binary=True)

"""# Train"""

#TODO: finish get data
def get_train_data():
  #todo
  return "",""

x_train, y_train = get_train_data()

# get the 5% of the data for semantic noise generation
amount = int(len(x_train)*0.005)
x_train = x_train[:amount]
y_train = y_train[:amount]

epochs = 200
history1 = m.fit(x_train, y_train, epochs = epochs, batch_size = b, verbose=1, validation_split=0.8, callbacks=callbacks)
history2 = m2.fit(x_train, y_train, epochs = epochs, batch_size = b, verbose=1, validation_split=0.8, callbacks=callbacks)
history3 = m3.fit(x_train, y_train, epochs = epochs, batch_size = b, verbose=1, validation_split=0.8, callbacks=callbacks)