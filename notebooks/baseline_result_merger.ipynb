{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "result_merger.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ze6dgSr9gUfF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "# -*- coding: utf-8 -*-\n",
        "#torch imports\n",
        "import torch\n",
        "torch.set_printoptions(profile=\"full\")\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "#some metrics\n",
        "from sklearn.metrics import recall_score, precision_score, f1_score\n",
        "#general imports\n",
        "import os\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/chexpert_semantic_noise/noisy_data/baseline')\n",
        "import h5py\n",
        "import time\n",
        "import json\n",
        "import pickle\n",
        "import shutil\n",
        "import datetime\n",
        "import threading\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "# some local files\n",
        "from tf_chexpert_utilities import *\n",
        "from torch_chexpert_dataset import ChexpertDataset, MetricKeeper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQap2tPb-g1L",
        "colab_type": "text"
      },
      "source": [
        "# One Hit - Finish Concatenate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OK7ZTctZ2dCB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def concatenate_results():\n",
        "  NEGATIVE_CASES = ['Enlarged Cardiomediastinum', \n",
        "                  'Cardiomegaly', \n",
        "                  'Lung Opacity', \n",
        "                  'Lung Lesion',\n",
        "                  'Edema',\n",
        "                  'Consolidation',\n",
        "                  'Pneumonia',\n",
        "                  'Atelectasis',\n",
        "                  'Pneumothorax',\n",
        "                  'Pleural Effusion',\n",
        "                  'Pleural Other',\n",
        "                  'Fracture',\n",
        "                  'Support Devices']\n",
        "\n",
        "  m_list = ['loss', \n",
        "            'acc', \n",
        "            'real_loss', \n",
        "            'real_acc', \n",
        "            'recall', \n",
        "            'precision', \n",
        "            'f_one_score',\n",
        "            'positive_acc',\n",
        "            'negative_acc',\n",
        "            'positive_real_acc',\n",
        "            'negative_real_acc']\n",
        "\n",
        "  # finish metric keeper\n",
        "\n",
        "  train_m_list = []\n",
        "  val_m_list = []\n",
        "\n",
        "  for m in m_list:\n",
        "    train_m_list.append(f'train_{m}'.lower())\n",
        "    val_m_list.append(f'val_{m}'.lower())\n",
        "  for n in NEGATIVE_CASES:\n",
        "    k = n.lower()\n",
        "    k = k.replace(' ', '_').lower()\n",
        "    train_m_list.append(f'train_{k}_acc')\n",
        "    train_m_list.append(f'train_{k}_real_acc')\n",
        "    val_m_list.append(f'val_{k}_acc')\n",
        "    val_m_list.append(f'val_{k}_real_acc')\n",
        "\n",
        "  print(train_m_list)\n",
        "  print(val_m_list)\n",
        "\n",
        "  source_folder = './network_training_predictions/torch_baseline'\n",
        "  destination_folder = './network_training_predictions/torch_baseline/0'\n",
        "\n",
        "  concatenated_metrics_e = {}\n",
        "  concatenated_metrics_i = {}\n",
        "  for m in train_m_list:\n",
        "    te = f'{source_folder}/{m}_e'\n",
        "    ti = f'{source_folder}/{m}_i'\n",
        "    concatenated_metrics_e[te] = np.zeros(50)\n",
        "    concatenated_metrics_i[ti] = np.zeros(419)\n",
        "\n",
        "  for m in val_m_list:\n",
        "    te = f'{source_folder}/{m}_e'\n",
        "    ti = f'{source_folder}/{m}_i'\n",
        "    concatenated_metrics_e[te] = np.zeros(50)\n",
        "    concatenated_metrics_i[ti] = np.zeros(52)\n",
        "\n",
        "  print('[TRAIN] concatenating individual results...')\n",
        "  for m in train_m_list:\n",
        "    t = f'{source_folder}/{m}_e'\n",
        "    print(f'{t}...')\n",
        "    try:\n",
        "      for i in range(50):\n",
        "        fname = f'{t}_{i}'\n",
        "        concatenated_metrics_e[t][i] = unpickle(fname)\n",
        "    except Exception as e:\n",
        "      print(f'warning: {e}')\n",
        "  print('[VAL] concatenating individual results...')\n",
        "  for m in val_m_list:\n",
        "    t = f'{source_folder}/{m}_e'\n",
        "    print(f'{t}...')\n",
        "    try:\n",
        "      for i in range(50):\n",
        "        fname = f'{t}_{i}'\n",
        "        concatenated_metrics_e[t][i] = unpickle(fname)\n",
        "    except Exception as e:\n",
        "      print(f'warning: {e}')\n",
        "\n",
        "  print('[TRAIN] saving concatenated results...')\n",
        "  for m in train_m_list:\n",
        "    source      = f'{source_folder}/{m}_e'\n",
        "    destination = f'{destination_folder}/{m}_e'\n",
        "    save_ndarray(destination, concatenated_metrics_e[source])\n",
        "    print(f'saved from: {source} to: {destination}...')\n",
        "\n",
        "  print('[VAL] saving concatenated results...')\n",
        "  for m in val_m_list:\n",
        "    source      = f'{source_folder}/{m}_e'\n",
        "    destination = f'{destination_folder}/{m}_e'\n",
        "    save_ndarray(destination, concatenated_metrics_e[source])\n",
        "    print(f'saved from: {source} to: {destination}...')\n",
        "\n",
        "  print('REMOVING UNNECESSARY FILES ...')\n",
        "  x = os.listdir(source_folder)\n",
        "  files = []\n",
        "  for i in x:\n",
        "    if 'train_' in i or 'val_' in i:\n",
        "      files.append(i)\n",
        "      \n",
        "  for f in files:\n",
        "    fname = f'{source_folder}/{f}'\n",
        "    open(fname).close()\n",
        "    os.remove(fname)"
      ],
      "execution_count": 3,
      "outputs": []
    }
  ]
}