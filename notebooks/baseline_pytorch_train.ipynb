{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_train.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "J9we1PCeFS23",
        "1GVHe_4pyLbC"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "67e1a4487c1243aba149f8947187be79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f91134aee0a9435986acace02dce22a3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_593289cd328440c181a77738fe483042",
              "IPY_MODEL_1850d7df75e74be9bb71831f631bf06f"
            ]
          }
        },
        "f91134aee0a9435986acace02dce22a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "593289cd328440c181a77738fe483042": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ae0e6b45744f4d7f98c0b5017263da97",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 32342954,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 32342954,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ed488b63657a47cb91ba6e8861680c93"
          }
        },
        "1850d7df75e74be9bb71831f631bf06f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b0299a1d7dcb4048b440913f47421b37",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 30.8M/30.8M [00:00&lt;00:00, 56.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_52192136ebf74ca98915069b990cf443"
          }
        },
        "ae0e6b45744f4d7f98c0b5017263da97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ed488b63657a47cb91ba6e8861680c93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b0299a1d7dcb4048b440913f47421b37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "52192136ebf74ca98915069b990cf443": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxGcT6oWqNx6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "2190dfea-188c-46ad-dfe0-a3a1a9c2be87"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"chexpert_noisfy.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1Up66s8R2xK4yRqBX0sGYIZpyZ1ncOnhd\n",
        "\"\"\"\n",
        "\n",
        "#torch imports\n",
        "import torch\n",
        "torch.set_printoptions(profile=\"full\")\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "from torch.autograd import Variable\n",
        "\n",
        "#general imports\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import shutil\n",
        "import datetime\n",
        "import threading\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "x = '/content/gdrive/My\\ Drive/chexpert_semantic_noise/noisy_data'\n",
        "%cd $x\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/chexpert_semantic_noise/noisy_data\n",
            " acc_0.jpg\t\t\t\t negative_real_acc_0.jpg\n",
            "'Atelectasis ACC_0.jpg'\t\t\t network_training_predictions\n",
            " baseline\t\t\t\t _old_images\n",
            " buffer\t\t\t\t\t'Pleural Effusion ACC_0.jpg'\n",
            "'Cardiomegaly ACC_0.jpg'\t\t'Pleural Other ACC_0.jpg'\n",
            "'Consolidation ACC_0.jpg'\t\t'Pneumonia ACC_0.jpg'\n",
            "'Edema ACC_0.jpg'\t\t\t'Pneumothorax ACC_0.jpg'\n",
            "'Enlarged Cardiomediastinum ACC_0.jpg'\t positive_acc_0.jpg\n",
            " forwardt\t\t\t\t positive_negative_acc_0.jpg\n",
            "'Fracture ACC_0.jpg'\t\t\t positive_real_acc_0.jpg\n",
            " loss_0.jpg\t\t\t\t real_acc_0.jpg\n",
            "'Lung Lesion ACC_0.jpg'\t\t\t real_loss_0.jpg\n",
            "'Lung Opacity ACC_0.jpg'\t\t'Support Devices ACC_0.jpg'\n",
            " models\t\t\t\t\t tensorboard_logs\n",
            " negative_acc_0.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSl_j9looe-I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5fd60cb4-0385-40ae-d06d-fd8bebd97328"
      },
      "source": [
        "!cat ./baseline/tgf_chexpert_utilities.py"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cat: ./baseline/tgf_chexpert_utilities.py: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dufkEDt3SIO",
        "colab_type": "text"
      },
      "source": [
        "# INITIALIZE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jiaD5Cr_Rbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append('./baseline')\n",
        "import h5py\n",
        "import time\n",
        "import json\n",
        "import pickle\n",
        "import datetime\n",
        "import threading\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "#torch imports\n",
        "import torch\n",
        "torch.set_printoptions(profile=\"full\")\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# some metrics\n",
        "from sklearn.metrics import recall_score, precision_score, f1_score\n",
        "# some local files\n",
        "#sys.path.append('./baseline')\n",
        "from tf_chexpert_utilities import *\n",
        "from torch_chexpert_dataset import ChexpertDataset, MetricKeeper"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DK7_tYubc99I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "b4eef8c2-b8aa-49ce-fb7a-26521817224d"
      },
      "source": [
        "def fpath(folder, noise):\n",
        "    return '%s/n_%s'%(folder, str(noise))\n",
        "\n",
        "\"\"\"## model compile func\"\"\"\n",
        "def compile_model(model, binary=False):\n",
        "  # Instantiate a logistic loss function that expects integer targets.\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  # Instantiate an accuracy metric.\n",
        "  accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "  if binary:\n",
        "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    accuracy = tf.keras.metrics.binary_accuracy\n",
        "  \n",
        "  # Instantiate an optimizer.\n",
        "  optimizer = tf.keras.optimizers.Adam()\n",
        "  # Instantiate some callbacks\n",
        "  \n",
        "  model.compile(optimizer=optimizer, loss=loss,metrics=[accuracy])\n",
        "  return model\n",
        "\n",
        "def initialize_logger(fname, train_length, val_length, test_length):\n",
        "    if os.path.exists(fname):\n",
        "      # if log file exists remove it permanately\n",
        "      return h5py.File(fname, 'a')\n",
        "\n",
        "    logger = h5py.File(fname, 'a')\n",
        "    logger.create_dataset('train_loss', (train_length, ), compression='gzip')\n",
        "    logger.create_dataset('train_real_loss', (train_length, ), compression='gzip')\n",
        "    logger.create_dataset('train_acc', (train_length, ), compression='gzip')\n",
        "    logger.create_dataset('train_real_acc', (train_length, ), compression='gzip')\n",
        "    logger.create_dataset('train_precision', (train_length, ), compression='gzip')\n",
        "    logger.create_dataset('train_recall', (train_length, ), compression='gzip')\n",
        "    logger.create_dataset('train_f1_score', (train_length, ), compression='gzip')\n",
        "\n",
        "    logger.create_dataset('val_loss', (train_length, ), compression='gzip')\n",
        "    logger.create_dataset('val_real_loss', (train_length, ), compression='gzip')\n",
        "    logger.create_dataset('val_acc', (train_length, ), compression='gzip')\n",
        "    logger.create_dataset('val_real_acc', (train_length, ), compression='gzip')\n",
        "    logger.create_dataset('val_precision', (train_length, ), compression='gzip')\n",
        "    logger.create_dataset('val_recall', (train_length, ), compression='gzip')\n",
        "    logger.create_dataset('val_f1_score', (train_length, ), compression='gzip')\n",
        "    return logger\n",
        "\n",
        "def iteration_log_metrics(KEY, iteration, loss, real_loss, acc, real_acc, precision, recall, _f1Score, \n",
        "                          positive_acc, positive_real_acc, negative_acc, negative_real_acc):\n",
        "   \n",
        "    if prediction_save_folder == None:\n",
        "      print('unable to log iteration \"prediction_save_folder\" variable doesnt exist.')\n",
        "      return\n",
        "  \n",
        "    save_ndarray(f'{prediction_save_folder}/{KEY}_loss_i_{iteration}', loss)\n",
        "    save_ndarray(f'{prediction_save_folder}/{KEY}_real_loss_i_{iteration}', real_loss)\n",
        "    save_ndarray(f'{prediction_save_folder}/{KEY}_acc_i_{iteration}', acc)\n",
        "    save_ndarray(f'{prediction_save_folder}/{KEY}_real_acc_i_{iteration}', real_acc)\n",
        "    save_ndarray(f'{prediction_save_folder}/{KEY}_precision_i_{iteration}', precision)\n",
        "    save_ndarray(f'{prediction_save_folder}/{KEY}_recall_i_{iteration}', recall)\n",
        "    save_ndarray(f'{prediction_save_folder}/{KEY}_f1_score_i_{iteration}', _f1Score)\n",
        "\n",
        "    save_ndarray(f'{prediction_save_folder}/{KEY}_positive_acc_i_{iteration}', positive_acc)\n",
        "    save_ndarray(f'{prediction_save_folder}/{KEY}_positive_real_acc_i_{iteration}', positive_real_acc)\n",
        "    save_ndarray(f'{prediction_save_folder}/{KEY}_negative_acc_i_{iteration}', negative_acc)\n",
        "    save_ndarray(f'{prediction_save_folder}/{KEY}_negative_real_acc_i_{iteration}', negative_real_acc)\n",
        "\n",
        "def log_metric_keeper(metric_keeper, epoch):\n",
        "  for name in metric_keeper.metric_name_list:\n",
        "    is_negative_case = False\n",
        "    for n in NEGATIVE_CASES:\n",
        "      k = n.lower()\n",
        "      k = k.replace(' ', '_').lower()\n",
        "      if k in name:\n",
        "        is_negative_case = True\n",
        "        break\n",
        "    if not is_negative_case:\n",
        "      save_ndarray(f'{prediction_save_folder}/{name}_e_{epoch}',metric_keeper.get(name, epoch))\n",
        "    else:\n",
        "      save_ndarray(f'{prediction_save_folder}/{name}_e_{epoch}',metric_keeper.get_case(name, epoch))\n",
        "\n",
        "def log_metrics(logger_file, KEY, EPOCH, loss, real_loss, acc, real_acc, precision, recall, _f1Score):\n",
        "    save_ndarray(f'{prediction_save_folder}/{KEY}_loss_e_{EPOCH}', loss)\n",
        "    save_ndarray(f'{prediction_save_folder}/{KEY}_real_loss_e_{EPOCH}', real_loss)\n",
        "    save_ndarray(f'{prediction_save_folder}/{KEY}_acc_e_{EPOCH}', acc)\n",
        "    save_ndarray(f'{prediction_save_folder}/{KEY}_real_acc_e_{EPOCH}', real_acc)\n",
        "    save_ndarray(f'{prediction_save_folder}/{KEY}_precision_e_{EPOCH}', precision)\n",
        "    save_ndarray(f'{prediction_save_folder}/{KEY}_recall_e_{EPOCH}', recall)\n",
        "    save_ndarray(f'{prediction_save_folder}/{KEY}_f1_score_e_{EPOCH}', _f1Score)\n",
        "\n",
        "    #logger_file[f'{KEY}_loss'][EPOCH]       = loss\n",
        "    #logger_file[f'{KEY}_real_loss'][EPOCH]  = real_loss\n",
        "    #logger_file[f'{KEY}_acc'][EPOCH]        = acc\n",
        "    #logger_file[f'{KEY}_real_acc'][EPOCH]   = real_acc\n",
        "    #logger_file[f'{KEY}_precision'][EPOCH]  = precision\n",
        "    #logger_file[f'{KEY}_recall'][EPOCH]     = recall\n",
        "    #logger_file[f'{KEY}_f1_score'][EPOCH]   = _f1Score\n",
        "\n",
        "\"\"\"## GET SAVED HDF5 DATASET FILE\"\"\"    \n",
        "# get h5py DATASET file\n",
        "dset = h5py.File('./buffer/baseline/dset.hdf5', 'r')\n",
        "train_dataset = dset['train']\n",
        "train_label_dataset = dset['train_label']\n",
        "val_dataset = dset['val']\n",
        "val_label_dataset = dset['val_label']\n",
        "test_dataset = dset['test']\n",
        "test_label_dataset = dset['test_label']\n",
        "\n",
        "\"\"\"## SHUFFLING INDICES\"\"\"\n",
        "# SHUFFLE TRAIN INDICES\n",
        "np.random.seed(1234)\n",
        "train_shuffled_idx_lst = np.arange(len(train_dataset))\n",
        "np.random.shuffle(train_shuffled_idx_lst)\n",
        "print(\"shuffled_train_indices: %s\"%str(train_shuffled_idx_lst))\n",
        "# SHUFFLE VAL INDICES\n",
        "np.random.seed(5678)\n",
        "val_shuffled_idx_lst = np.arange(len(val_dataset))\n",
        "np.random.shuffle(val_shuffled_idx_lst)\n",
        "print(\"shuffled_val_indices: %s\"%str(val_shuffled_idx_lst))\n",
        "# SHUFFLE TEST INDICES\n",
        "np.random.seed(9012)\n",
        "test_shuffled_idx_lst = np.arange(len(test_dataset))\n",
        "np.random.shuffle(test_shuffled_idx_lst)\n",
        "print(\"shuffled_test_indices: %s\"%str(test_shuffled_idx_lst))\n",
        "print('-'*30)\n",
        "\n",
        "\"\"\"## GETTING THE GROUND TRUTH AND NOISY LABELS\"\"\"\n",
        "train_noisy_label_dict  = {}\n",
        "val_noisy_label_dict    = {}\n",
        "train_ground_truth      = get_noisy_labels(0.0, 'train', len(train_dataset))\n",
        "val_ground_truth        = get_noisy_labels(0.0, 'val', len(val_dataset))\n",
        "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  - - - - - - - - \n",
        "noises = [0, 0.1, 0.2, 0.3, 0.4, 0.5, .60]\n",
        "for n in noises:\n",
        "  train_noisy_label_dict[n] = get_noisy_labels(n, 'train', len(train_dataset))\n",
        "  val_noisy_label_dict[n]   = get_noisy_labels(n, 'val', len(val_dataset))\n",
        "\n",
        "\n",
        "\"\"\"## SOME TRAINING PARAMATERS\"\"\"\n",
        "# get custom datasets for network training\n",
        "\n",
        "# GET SHUFFLED NAMES/LABES FOR TRAIN\n",
        "train_ground_truth  = train_ground_truth[train_shuffled_idx_lst]\n",
        "val_ground_truth    = val_ground_truth[val_shuffled_idx_lst]\n",
        "for n in noises:\n",
        "    train_noisy_label_dict[n]   =  train_noisy_label_dict[n][train_shuffled_idx_lst]\n",
        "    val_noisy_label_dict[n]     =  val_noisy_label_dict[n][val_shuffled_idx_lst]\n",
        "\n",
        "tensorboard_log_dir = './tensorboard_logs'\n",
        "network_training_pred_folder = './network_training_predictions'\n",
        "prediction_save_folder = './network_training_predictions/torch_baseline'\n",
        "model_dir = './models'\n",
        "model_save_dir = './models/torch_baseline'\n",
        "\n",
        "make_sure_folder_exists(tensorboard_log_dir)\n",
        "make_sure_folder_exists(network_training_pred_folder)\n",
        "make_sure_folder_exists(prediction_save_folder)\n",
        "make_sure_folder_exists(model_dir)\n",
        "make_sure_folder_exists(model_save_dir)\n",
        "make_sure_folder_exists('%s/%s'%(prediction_save_folder, '0'))\n",
        "for n in noises:\n",
        "    make_sure_folder_exists('%s/%i'%(prediction_save_folder, int(n*100)))\n",
        "\n",
        "if (train_ground_truth == train_noisy_label_dict[0]).all():\n",
        "  print('Train labels are correct')\n",
        "else:\n",
        "  print('Train labels are NOT CORRECT')\n",
        "\n",
        "if (val_ground_truth == val_noisy_label_dict[0]).all():\n",
        "  print('Validation labels are correct')\n",
        "else:\n",
        "  print('Validation labels are NOT CORRECT')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shuffled_train_indices: [14025 11750 13068 ... 23605  1318 25299]\n",
            "shuffled_val_indices: [  93 1854 2937 ... 2366 2199 2164]\n",
            "shuffled_test_indices: [ 831 3024 2751 ... 3302 2522 3314]\n",
            "------------------------------\n",
            "Given folder: ./tensorboard_logs EXISTS.\n",
            "Given folder: ./network_training_predictions EXISTS.\n",
            "Given folder: ./network_training_predictions/torch_baseline EXISTS.\n",
            "Given folder: ./models EXISTS.\n",
            "Given folder: ./models/torch_baseline EXISTS.\n",
            "Given folder: ./network_training_predictions/torch_baseline/0 EXISTS.\n",
            "Given folder: ./network_training_predictions/torch_baseline/0 EXISTS.\n",
            "Given folder: ./network_training_predictions/torch_baseline/10 EXISTS.\n",
            "Given folder: ./network_training_predictions/torch_baseline/20 EXISTS.\n",
            "Given folder: ./network_training_predictions/torch_baseline/30 EXISTS.\n",
            "Given folder: ./network_training_predictions/torch_baseline/40 EXISTS.\n",
            "Given folder: ./network_training_predictions/torch_baseline/50 EXISTS.\n",
            "Given folder: ./network_training_predictions/torch_baseline/60 EXISTS.\n",
            "Train labels are correct\n",
            "Validation labels are correct\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiyWPxSjFcrC",
        "colab_type": "text"
      },
      "source": [
        "# SET ARGS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3MknknQpr9O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzKs-eUddBpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get args\n",
        "# #Â #Â #Â if you are using a notebook comment the code below and uncomment the ' NOETBOOK MODE ' code block# #Â #Â #\n",
        "#opt = get_args()\n",
        "\n",
        "#BATCH_SIZE = opt.batch_size\n",
        "#MAX_EPOCHS = opt.max_epoch\n",
        "#NOISE_RATIO = opt.noise_ratio\n",
        "\n",
        "#LR = opt.lr\n",
        "#MOMENTUM = opt.momentum\n",
        "#WEIGHT_DECAY = opt.weight_decay\n",
        "#RESUME = opt.resume\n",
        "skip_mean = True\n",
        "# #Â #Â # NOTEBOOK MODE # #Â #Â #\n",
        "BATCH_SIZE = 32\n",
        "TRANSFORM_RESIZE = 224\n",
        "MAX_EPOCHS = 50\n",
        "NOISE_RATIO = 0\n",
        "LR = 5e-3\n",
        "LR_DECAY = 0.1\n",
        "LR_DECAY_PER_EPOCH = 10\n",
        "MOMENTUM = 0.9 \n",
        "B=0.99\n",
        "WEIGHT_DECAY = 1e-4\n",
        "LOG_ITERATION_FACTOR = 100\n",
        "RESUME = 0\n",
        "#skip_mean = True\n",
        "def get_str(item):\n",
        "  lr_f  = len(str(LR)) - 2\n",
        "  coeff = round(LR/(10**-lr_f))\n",
        "  return f'[{coeff}e-{lr_f}]'\n",
        "\n",
        "def get_checkpoint_fname():\n",
        "  s_lr = f'lr_{get_str(LR)}'\n",
        "  s_wd = f'wd_{get_str(WEIGHT_DECAY)}'\n",
        "  s_b  = f'b_{BATCH_SIZE}' \n",
        "  return f'{s_lr}_{s_wd}_{s_b}_checkpoint.pth.tar'\n",
        "# open h5py file to save the epoch states\n",
        "#logger = initialize_logger('%s/%i/log.hdf5'%(prediction_save_folder, int(100*NOISE_RATIO)), len(train_dataset), len(val_dataset), len(test_dataset))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ7lq6L0qcBf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "4a4bf74f-93bc-445d-b958-ed407bf9cf65"
      },
      "source": [
        "get_checkpoint_fname()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'lr_[5e-3]_wd_[5e-3]_b_32_checkpoint.pth.tar'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCx3vbroOPfc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8cbeb8cb-f9da-4ece-c5c6-b348e7798566"
      },
      "source": [
        "MAX_EPOCHS"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpFScDFPGMmI",
        "colab_type": "text"
      },
      "source": [
        "# TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYT3LGNTX3Ug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np_train_labal_dataset = np.array(train_label_dataset, dtype='object')[train_shuffled_idx_lst]\n",
        "np_val_labal_dataset = np.array(val_label_dataset, dtype='object')[val_shuffled_idx_lst]\n",
        "np_test_labal_dataset = np.array(test_label_dataset, dtype='object')[test_shuffled_idx_lst]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NmgSYWqgmdJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np_train_dataset = np.array(train_dataset, dtype=np.uint8)[train_shuffled_idx_lst]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5oUZs2TsD_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np_val_dataset = np.array(val_dataset, dtype=np.uint8)[val_shuffled_idx_lst]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDsDG1fBsFWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np_test_dataset = np.array(test_dataset, dtype=np.uint8)[test_shuffled_idx_lst]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CU1pvHuAdiLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not skip_mean:\n",
        "  # Get mean to initialize transform\n",
        "  trainset = ChexpertDataset(r=NOISE_RATIO, hdf5_dataset=np_train_dataset, noisy_labels=train_noisy_label_dict[NOISE_RATIO], batch_size=BATCH_SIZE, ground_truth=train_ground_truth, transform=transforms.ToTensor())\n",
        "  trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "  mean = 0\n",
        "  before = datetime.datetime.now()    \n",
        "  for i, data in enumerate(trainloader, 0):\n",
        "      imgs, labels, real_labels, index= data\n",
        "      mean += torch.from_numpy(np.mean(np.asarray(imgs), axis=(2,3))).sum(0)\n",
        "      if (i+1)%15 == 0:\n",
        "          print_remaining_time(before, i+1, len(trainloader))\n",
        "  mean = mean / len(trainset)\n",
        "else:\n",
        "  mean = [0.4365, 0.4365, 0.4365]\n",
        "# get transform and initialize trainset and trainloader\n",
        "# todo: normalize 0 to 1 for both training and testing\n",
        "transform_train = transforms.Compose([transforms.Resize(TRANSFORM_RESIZE),\n",
        "                                      transforms.RandomCrop(TRANSFORM_RESIZE, padding=4), \n",
        "                                      transforms.RandomHorizontalFlip(), \n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((mean[0], mean[1], mean[2]),(1.0, 1.0, 1.0))])\n",
        "                                      \n",
        "transform_test = transforms.Compose([transforms.Resize(TRANSFORM_RESIZE),\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize((mean[0], mean[1], mean[2]), (1.0, 1.0, 1.0))])\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Bpm287Dz-lq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9d72ebcc-f519-4a3d-c73c-e3d7d3bcf82b"
      },
      "source": [
        "print(f'mean: {mean}')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean: [0.4365, 0.4365, 0.4365]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M62f5Pe0f5yJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "136a2253-ebb6-41a2-c023-b9f5b1896cea"
      },
      "source": [
        "trainset = ChexpertDataset(r=NOISE_RATIO, hdf5_dataset=np_train_dataset, noisy_labels=train_noisy_label_dict[NOISE_RATIO], batch_size=BATCH_SIZE, ground_truth=train_ground_truth, transform=transform_train)\n",
        "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "valset = ChexpertDataset(r=NOISE_RATIO, hdf5_dataset=np_val_dataset, noisy_labels=val_noisy_label_dict[NOISE_RATIO], batch_size=BATCH_SIZE, ground_truth=val_ground_truth, transform=transform_test)\n",
        "valloader = DataLoader(valset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "print(f'trainset length: {len(trainset)}')\n",
        "print(f'train loader length: {len(trainloader)}')\n",
        "print(f'valset length: {len(valset)}')\n",
        "print(f'val loader length: {len(valloader)}')\n",
        "TRAINLOADER_LENGTH = len(trainloader)\n",
        "VALLOADER_LENGTH = len(valloader)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trainset length: 26624\n",
            "train loader length: 832\n",
            "valset length: 3328\n",
            "val loader length: 104\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lN31yf1HE80",
        "colab_type": "text"
      },
      "source": [
        "# LOAD AND TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO1bFmis_yfI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "net = torch.hub.load('pytorch/vision:v0.6.0', 'densenet121', pretrained=True)\n",
        "print(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFW5Cjh7dix3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "b3ddf2e5-7643-4dfc-d64d-1455d5ee39fa"
      },
      "source": [
        "NEGATIVE_CASES = ['Enlarged Cardiomediastinum', \n",
        "                  'Cardiomegaly', \n",
        "                  'Lung Opacity', \n",
        "                  'Lung Lesion',\n",
        "                  'Edema',\n",
        "                  'Consolidation',\n",
        "                  'Pneumonia',\n",
        "                  'Atelectasis',\n",
        "                  'Pneumothorax',\n",
        "                  'Pleural Effusion',\n",
        "                  'Pleural Other',\n",
        "                  'Fracture',\n",
        "                  'Support Devices']\n",
        "\n",
        "m_list = ['loss', \n",
        "          'acc', \n",
        "          'real_loss', \n",
        "          'real_acc', \n",
        "          'recall', \n",
        "          'precision', \n",
        "          'f_one_score',\n",
        "          'positive_acc',\n",
        "          'negative_acc',\n",
        "          'positive_real_acc',\n",
        "          'negative_real_acc']\n",
        "\n",
        "# finish metric keeper\n",
        "\n",
        "train_m_list = []\n",
        "val_m_list = []\n",
        "\n",
        "for m in m_list:\n",
        "  train_m_list.append(f'train_{m}'.lower())\n",
        "  val_m_list.append(f'val_{m}'.lower())\n",
        "for n in NEGATIVE_CASES:\n",
        "  k = n.lower()\n",
        "  k = k.replace(' ', '_').lower()\n",
        "  train_m_list.append(f'train_{k}_acc')\n",
        "  train_m_list.append(f'train_{k}_real_acc')\n",
        "  val_m_list.append(f'val_{k}_acc')\n",
        "  val_m_list.append(f'val_{k}_real_acc')\n",
        "\n",
        "# - //\n",
        "\n",
        "train_metric_keeper = MetricKeeper(TRAINLOADER_LENGTH, MAX_EPOCHS, train_m_list)\n",
        "val_metric_keeper = MetricKeeper(VALLOADER_LENGTH, MAX_EPOCHS, val_m_list)\n",
        "\n",
        "for n in NEGATIVE_CASES:\n",
        "  k = n.lower().replace(' ', '_').lower()\n",
        "  # - #\n",
        "  train_metric_keeper.reset_case(f'train_{k}_acc')\n",
        "  train_metric_keeper.reset_case(f'train_{k}_real_acc')\n",
        "  val_metric_keeper.reset_case(f'val_{k}_acc')\n",
        "  val_metric_keeper.reset_case(f'val_{k}_real_acc')\n",
        "\n",
        "train_metric_keeper.list_names()\n",
        "val_metric_keeper.list_names()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "METRIC KEEPER - METRIC NAMES: ['train_loss', 'train_acc', 'train_real_loss', 'train_real_acc', 'train_recall', 'train_precision', 'train_f_one_score', 'train_positive_acc', 'train_negative_acc', 'train_positive_real_acc', 'train_negative_real_acc', 'train_enlarged_cardiomediastinum_acc', 'train_enlarged_cardiomediastinum_real_acc', 'train_cardiomegaly_acc', 'train_cardiomegaly_real_acc', 'train_lung_opacity_acc', 'train_lung_opacity_real_acc', 'train_lung_lesion_acc', 'train_lung_lesion_real_acc', 'train_edema_acc', 'train_edema_real_acc', 'train_consolidation_acc', 'train_consolidation_real_acc', 'train_pneumonia_acc', 'train_pneumonia_real_acc', 'train_atelectasis_acc', 'train_atelectasis_real_acc', 'train_pneumothorax_acc', 'train_pneumothorax_real_acc', 'train_pleural_effusion_acc', 'train_pleural_effusion_real_acc', 'train_pleural_other_acc', 'train_pleural_other_real_acc', 'train_fracture_acc', 'train_fracture_real_acc', 'train_support_devices_acc', 'train_support_devices_real_acc']\n",
            "METRIC KEEPER - METRIC NAMES: ['val_loss', 'val_acc', 'val_real_loss', 'val_real_acc', 'val_recall', 'val_precision', 'val_f_one_score', 'val_positive_acc', 'val_negative_acc', 'val_positive_real_acc', 'val_negative_real_acc', 'val_enlarged_cardiomediastinum_acc', 'val_enlarged_cardiomediastinum_real_acc', 'val_cardiomegaly_acc', 'val_cardiomegaly_real_acc', 'val_lung_opacity_acc', 'val_lung_opacity_real_acc', 'val_lung_lesion_acc', 'val_lung_lesion_real_acc', 'val_edema_acc', 'val_edema_real_acc', 'val_consolidation_acc', 'val_consolidation_real_acc', 'val_pneumonia_acc', 'val_pneumonia_real_acc', 'val_atelectasis_acc', 'val_atelectasis_real_acc', 'val_pneumothorax_acc', 'val_pneumothorax_real_acc', 'val_pleural_effusion_acc', 'val_pleural_effusion_real_acc', 'val_pleural_other_acc', 'val_pleural_other_real_acc', 'val_fracture_acc', 'val_fracture_real_acc', 'val_support_devices_acc', 'val_support_devices_real_acc']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FrB9Cp8Zgx9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "be521f06-da2d-4984-b357-45b3393eeda0"
      },
      "source": [
        "print(train_metric_keeper.get_case('train_enlarged_cardiomediastinum_acc', 0))\n",
        "train_metric_keeper.get('train_acc',0)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "./baseline/torch_chexpert_dataset.py:95: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return np.divide(summed_values, summed_counts)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtwcRI0D9xPt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "67e1a4487c1243aba149f8947187be79",
            "f91134aee0a9435986acace02dce22a3",
            "593289cd328440c181a77738fe483042",
            "1850d7df75e74be9bb71831f631bf06f",
            "ae0e6b45744f4d7f98c0b5017263da97",
            "ed488b63657a47cb91ba6e8861680c93",
            "b0299a1d7dcb4048b440913f47421b37",
            "52192136ebf74ca98915069b990cf443"
          ]
        },
        "outputId": "06686d70-78d9-414e-f351-c7b26e345473"
      },
      "source": [
        "print('************** SETUP PARAMETERS **************')\n",
        "print(f'BATCH_SIZE:{BATCH_SIZE}')\n",
        "print(f'TRANSFORM_RESIZE:{TRANSFORM_RESIZE}')\n",
        "print(f'MAX_EPOCHS:{MAX_EPOCHS}')\n",
        "print(f'NOISE_RATIO:{NOISE_RATIO}')\n",
        "print(f'LR:{LR}')\n",
        "print(f'LR_DECAY:{LR_DECAY}')\n",
        "print(f'LR_DECAY_PER_EPOCH:{LR_DECAY_PER_EPOCH}')\n",
        "print(f'MOMENTUM:{MOMENTUM}')\n",
        "print(f'WEIGHT_DECAY:{WEIGHT_DECAY}')\n",
        "print(f'LOG_ITERATION_FACTOR:{LOG_ITERATION_FACTOR}')\n",
        "print(f'RESUME:{RESUME}')\n",
        "print('**********************************************')\n",
        "\n",
        "# get densenet\n",
        "net = torch.hub.load('pytorch/vision:v0.6.0', 'densenet121', pretrained=True)\n",
        "\n",
        "# change the last classifier to predict 2 class instead of 10\n",
        "num_ftrs = net.classifier.in_features\n",
        "net.classifier = nn.Linear(in_features=num_ftrs, out_features=2, bias=True)\n",
        "\n",
        "#Â get criterion\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#####--###############--###############--#####\n",
        "net.cuda()\n",
        "criterion.cuda()\n",
        "#####--###############--###############--#####\n",
        "optimizer = optim.Adam(net.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "num_classes = 2\n",
        "\n",
        "if RESUME == 1:\n",
        "  fn = os.path.join(model_save_dir, get_checkpoint_fname())\n",
        "  ckpt = torch.load(fn)\n",
        "  epoch_resume = ckpt['epoch']\n",
        "  best_val_acc = ckpt['best_val_acc']\n",
        "  net.load_state_dict(ckpt['state_dict'])\n",
        "  optimizer.load_state_dict(ckpt['optimizer'])\n",
        "  print('loading network SUCCESSFUL')\n",
        "else:\n",
        "  epoch_resume = 0\n",
        "  best_val_acc = 0\n",
        "  print('Starting training WITHOUT loading a network...') \n",
        "\n",
        "# TRAINING\n",
        "for epoch in range(epoch_resume, MAX_EPOCHS):\n",
        "  print('*'*20 + f'TRAIN e:{epoch}' + '*'*20)\n",
        "  lr = LR * (LR_DECAY ** (epoch // LR_DECAY_PER_EPOCH))\n",
        "  print(f'Learning rate of epoch:{epoch} is: {lr}')\n",
        "  for param_group in optimizer.param_groups:\n",
        "      param_group['lr'] = lr\n",
        "\n",
        "  before_all = datetime.datetime.now()\n",
        "  before = datetime.datetime.now()\n",
        "  for i, data in enumerate(trainloader, 0):\n",
        "    total_iteration = TRAINLOADER_LENGTH * epoch + i\n",
        "    net.zero_grad()\n",
        "    imgs, labels, real_ground_truths, index = data\n",
        "    label_names = np_train_labal_dataset[index]\n",
        "        # imgs               : data\n",
        "        # labels             : noisy labels\n",
        "        # real ground truths : not-noisy ground truths\n",
        "        # index              : sample index\n",
        "    imgs = Variable(imgs.cuda().float())\n",
        "    labels = Variable(labels.cuda().long()[:,1])\n",
        "    real_ground_truths = Variable(real_ground_truths.cuda().long())\n",
        "    # forward\n",
        "    logits = net(imgs.float())\n",
        "    # get the accuracy and predictions of the current batch\n",
        "    _, pred = torch.max(logits.data, -1)\n",
        "    acc = float((pred==labels.data).sum()) \n",
        "    train_metric_keeper.add('train_acc', acc/BATCH_SIZE, total_iteration)\n",
        "    # get indices\n",
        "    positive_indices = np.where(labels.data.cpu() == 0)[0]\n",
        "    negative_indices = np.where(labels.data.cpu() == 1)[0]\n",
        "    positive_acc = float((pred[positive_indices]==labels.data[positive_indices]).sum())\n",
        "    negative_acc = float((pred[negative_indices]==labels.data[negative_indices]).sum())\n",
        "    train_metric_keeper.add('train_positive_acc', positive_acc/len(positive_indices), total_iteration)\n",
        "    train_metric_keeper.add('train_negative_acc', negative_acc/len(negative_indices), total_iteration)\n",
        "    # real acc\n",
        "    real_acc = float((pred==real_ground_truths.data[:,1]).sum())\n",
        "    train_metric_keeper.add('train_real_acc', real_acc/BATCH_SIZE, total_iteration)\n",
        "    real_positive_indices = np.where(real_ground_truths.data[:,1].cpu() == 0)[0]\n",
        "    real_negative_indices = np.where(real_ground_truths.data[:,1].cpu() == 1)[0]\n",
        "    real_positive_acc = float((pred[positive_indices]==real_ground_truths.data[:,1][positive_indices]).sum())\n",
        "    real_negative_acc = float((pred[negative_indices]==real_ground_truths.data[:,1][negative_indices]).sum())\n",
        "    train_metric_keeper.add('train_positive_real_acc', real_positive_acc/len(real_positive_indices), total_iteration)\n",
        "    train_metric_keeper.add('train_negative_real_acc', real_negative_acc/len(real_negative_indices), total_iteration)\n",
        "    # negative cases\n",
        "    for n in NEGATIVE_CASES:\n",
        "      case_indices = np.where(label_names == n)[0]\n",
        "      if len(case_indices) == 0:\n",
        "        continue\n",
        "\n",
        "      acc       = float((pred[case_indices]==labels.data[case_indices]).sum())\n",
        "      real_acc  = float((pred[case_indices]==real_ground_truths.data[:,1][case_indices]).sum())\n",
        "      k  = n.lower().replace(' ', '_').lower()      \n",
        "      t  = f'train_{k}_acc'\n",
        "      tt = f'train_{k}_real_acc'\n",
        "      train_metric_keeper.add_case(t, acc/len(case_indices), total_iteration)\n",
        "      train_metric_keeper.add_case(tt, real_acc/len(case_indices), total_iteration)\n",
        "\n",
        "    # get current loss of the batch\n",
        "    current_loss = criterion(logits, labels)\n",
        "    individual_loss = current_loss.data\n",
        "    train_metric_keeper.add('train_loss', individual_loss.cpu(), total_iteration)\n",
        "    # real loss\n",
        "    current_real_loss = criterion(logits, real_ground_truths[:,1])\n",
        "    individual_real_loss = current_real_loss.data\n",
        "    train_metric_keeper.add('train_real_loss', individual_real_loss.cpu(), total_iteration)\n",
        "    # backward\n",
        "    current_loss.backward()\n",
        "    optimizer.step()\n",
        "    # put prediction history in a list to save it later\n",
        "    softmax_pred        = F.softmax(logits, -1).cpu().data\n",
        "    #recall & prec & f1 score\n",
        "    individual_recall = recall_score(real_ground_truths.cpu(), softmax_pred.cpu() > 0.5,    average=\"samples\")\n",
        "    individual_precision = precision_score(real_ground_truths.cpu(), softmax_pred.cpu() > 0.5, average=\"samples\")\n",
        "    individual_f1_score = f1_score(real_ground_truths.cpu(), softmax_pred.cpu() > 0.5,        average=\"samples\")\n",
        "    # UPDATE METRIC KEEPER\n",
        "    train_metric_keeper.add('train_recall', individual_recall, total_iteration)\n",
        "    train_metric_keeper.add('train_precision', individual_precision, total_iteration)\n",
        "    train_metric_keeper.add('train_f_one_score', individual_f1_score, total_iteration)    \n",
        "    # LOGGING\n",
        "    if (total_iteration+1) % LOG_ITERATION_FACTOR == 0:\n",
        "      print(f'Logging train results of iteration: {i} on epoch:{epoch} with total itration: {total_iteration}')\n",
        "      iteration_log_metrics(\n",
        "        KEY='train', \n",
        "        iteration=total_iteration, \n",
        "        loss=individual_loss.cpu(),\n",
        "        real_loss=individual_real_loss.cpu(),\n",
        "        acc=acc/BATCH_SIZE,\n",
        "        real_acc=real_acc/BATCH_SIZE,\n",
        "        recall=individual_recall,\n",
        "        precision=individual_precision,\n",
        "        _f1Score = individual_f1_score,\n",
        "        positive_acc = positive_acc/len(positive_indices),\n",
        "        positive_real_acc = real_positive_acc/len(real_positive_indices),\n",
        "        negative_acc = negative_acc/len(negative_indices),\n",
        "        negative_real_acc = real_negative_acc/len(real_negative_indices))\n",
        "      print_remaining_time(before, i+1, TRAINLOADER_LENGTH)\n",
        "\n",
        "  print(f\"TRAIN EPOCH END [{epoch}/{MAX_EPOCHS}]\")\n",
        "  for n in NEGATIVE_CASES:\n",
        "    k  = n.lower().replace(' ', '_').lower()\n",
        "    t   = f'train_{k}_acc'\n",
        "    tt  = f'train_{k}_real_acc' \n",
        "    print(f\"{t}: \\t {train_metric_keeper.get_case(t,epoch)}\")\n",
        "    print(f\"{tt}: \\t {train_metric_keeper.get_case(tt,epoch)}\")\n",
        "  print(f\"Loss: \\t {train_metric_keeper.get('train_loss', epoch)}\")\n",
        "  print(f\"REAL LOSS: \\t {train_metric_keeper.get('train_real_loss', epoch)}\")\n",
        "  print(f\"ACC: \\t {train_metric_keeper.get('train_acc', epoch)}\")\n",
        "  print(f\"Positive ACC: \\t {train_metric_keeper.get('train_positive_acc', epoch)}\")\n",
        "  print(f\"Negative ACC: \\t {train_metric_keeper.get('train_negative_acc', epoch)}\")\n",
        "  print(f\"REAL ACC: \\t {train_metric_keeper.get('train_real_acc', epoch)}\")\n",
        "  print(f\"Positive REAL ACC: \\t {train_metric_keeper.get('train_positive_real_acc', epoch)}\")\n",
        "  print(f\"Negative REAL ACC: \\t {train_metric_keeper.get('train_negative_real_acc', epoch)}\")\n",
        "  print(f\"RECALL: \\t {train_metric_keeper.get('train_recall', epoch)}\")\n",
        "  print(f\"PRECISION: \\t {train_metric_keeper.get('train_precision', epoch)}\")\n",
        "  print(f\"F1_SCORE: \\t {train_metric_keeper.get('train_f_one_score', epoch)}\")\n",
        "  print('-'*100)\n",
        "  #loss: %5f, real_loss: %5f, acc: %5f, real_acc:%5f, recall: %5f, precision: %5f, f1_score: %5f' %)\n",
        "  print('*'*20+f'VALIDATION e:{epoch}'+'*'*20)\n",
        "  # SAVE THE METRICS\n",
        "  log_metric_keeper(train_metric_keeper, epoch)\n",
        "  # ---------- VALIDATION STATE ----------\n",
        "  net.eval()\n",
        "  best_val_acc = 0\n",
        "  before = datetime.datetime.now()\n",
        "  with torch.no_grad():\n",
        "    for i, data in enumerate(valloader, 0):\n",
        "      total_iteration = VALLOADER_LENGTH * epoch + i\n",
        "      # get data\n",
        "      imgs, labels, real_ground_truths, index = data\n",
        "      imgs = Variable(imgs.cuda())\n",
        "      labels = Variable(labels.cuda().long()[:,1])\n",
        "      label_names = np_val_labal_dataset[index]\n",
        "      real_ground_truths = Variable(real_ground_truths.cuda().long())\n",
        "      # LOSS and REAL LOSS\n",
        "      logits = net(imgs.float())\n",
        "      #current_loss\n",
        "      loss = criterion(logits, labels).data\n",
        "      val_metric_keeper.add('val_loss', loss.cpu(), total_iteration)\n",
        "      real_loss = criterion(logits, real_ground_truths[:,1]).data\n",
        "      val_metric_keeper.add('val_real_loss', real_loss.cpu(), total_iteration)\n",
        "      # ACC\n",
        "      _, pred = torch.max(logits.data, -1)\n",
        "      acc = float((pred==labels.data).sum())\n",
        "      val_metric_keeper.add('val_acc', acc/BATCH_SIZE, total_iteration)\n",
        "      # POSITIVE ACC AND NEGATIVE ACC\n",
        "      positive_indices = np.where(labels.data.cpu() == 0)[0]\n",
        "      negative_indices = np.where(labels.data.cpu() == 1)[0]\n",
        "      positive_acc = float((pred[positive_indices]==labels.data[positive_indices]).sum())\n",
        "      negative_acc = float((pred[negative_indices]==labels.data[negative_indices]).sum())\n",
        "      val_metric_keeper.add('val_positive_acc', positive_acc/len(positive_indices), total_iteration)\n",
        "      val_metric_keeper.add('val_negative_acc', negative_acc/len(negative_indices), total_iteration)\n",
        "      # REAL ACC\n",
        "      real_acc = float((pred==real_ground_truths.data[:,1]).sum())\n",
        "      val_metric_keeper.add('val_real_acc', real_acc/BATCH_SIZE, total_iteration)\n",
        "      # POSITIVE REAL ACC AND NEGATIVE REAL ACC\n",
        "      real_positive_indices = np.where(real_ground_truths.data[:,1].cpu() == 0)[0]\n",
        "      real_negative_indices = np.where(real_ground_truths.data[:,1].cpu() == 1)[0]\n",
        "      real_positive_acc = float((pred[positive_indices]==real_ground_truths.data[:,1][positive_indices]).sum())\n",
        "      real_negative_acc = float((pred[negative_indices]==real_ground_truths.data[:,1][negative_indices]).sum())\n",
        "      val_metric_keeper.add('val_positive_real_acc', real_positive_acc/len(real_positive_indices), total_iteration)\n",
        "      val_metric_keeper.add('val_negative_real_acc', real_negative_acc/len(real_negative_indices), total_iteration)\n",
        "\n",
        "      # negative cases\n",
        "      for n in NEGATIVE_CASES:\n",
        "        case_indices = np.where(label_names == n)[0]\n",
        "        if len(case_indices) == 0:\n",
        "          continue\n",
        "        acc       = float((pred[case_indices]==labels.data[case_indices]).sum())\n",
        "        real_acc  = float((pred[case_indices]==real_ground_truths.data[:,1][case_indices]).sum())\n",
        "        k = n.lower().replace(' ', '_').lower()\n",
        "        # save them _acc\n",
        "        t  = f'val_{k}_acc'\n",
        "        tt = f'val_{k}_real_acc'\n",
        "        val_metric_keeper.add_case(t, acc/len(case_indices), total_iteration)\n",
        "        val_metric_keeper.add_case(tt, real_acc/len(case_indices), total_iteration)\n",
        "\n",
        "      # put prediction history in a list to save it later\n",
        "      softmax_pred            = F.softmax(logits, -1).cpu().data\n",
        "      # RECALL & prec & f1 score\n",
        "      individual_recall = recall_score(real_ground_truths.cpu(), softmax_pred.cpu() > 0.5,    average=\"samples\")\n",
        "      individual_precision = precision_score(real_ground_truths.cpu(), softmax_pred.cpu() > 0.5, average=\"samples\")\n",
        "      individual_f1_score = f1_score(real_ground_truths.cpu(), softmax_pred.cpu() > 0.5,        average=\"samples\")\n",
        "      # update metric\n",
        "      val_metric_keeper.add('val_recall', individual_recall, total_iteration)\n",
        "      val_metric_keeper.add('val_precision', individual_precision, total_iteration)\n",
        "      val_metric_keeper.add('val_f_one_score', individual_f1_score, total_iteration)\n",
        "\n",
        "      if (total_iteration+1) % LOG_ITERATION_FACTOR == 0:\n",
        "        print(f'Logging VAL results of iteration: {i} on epoch:{epoch} with total itration: {total_iteration}')\n",
        "        iteration_log_metrics(\n",
        "          KEY='val', \n",
        "          iteration=total_iteration, \n",
        "          loss=loss.cpu(),\n",
        "          real_loss=real_loss.cpu(),\n",
        "          acc=acc/BATCH_SIZE,\n",
        "          real_acc=real_acc/BATCH_SIZE,\n",
        "          recall=individual_recall,\n",
        "          precision=individual_precision,\n",
        "          _f1Score = individual_f1_score,\n",
        "          positive_acc = positive_acc/len(positive_indices),\n",
        "          positive_real_acc = real_positive_acc/len(real_positive_indices),\n",
        "          negative_acc = negative_acc/len(negative_indices),\n",
        "          negative_real_acc = real_negative_acc/len(real_negative_indices))\n",
        "\n",
        "        print_remaining_time(before, i+1, VALLOADER_LENGTH)\n",
        "\n",
        "  ####################################################\n",
        "  val_acc = val_metric_keeper.get('val_acc', epoch)\n",
        "  is_best = val_acc > best_val_acc\n",
        "  best_val_acc = max(val_acc, best_val_acc)\n",
        "  # SAVE THE METRICS\n",
        "  log_metric_keeper(val_metric_keeper, epoch)\n",
        "  ####################################################\n",
        "  print(f\"VALIDATION EPOCH END [{epoch}/{MAX_EPOCHS}]\")\n",
        "  for n in NEGATIVE_CASES:\n",
        "    k  = n.lower().replace(' ', '_').lower()\n",
        "    t   = f'val_{k}_acc'\n",
        "    tt  = f'val_{k}_real_acc' \n",
        "    print(f\"{t}: \\t {val_metric_keeper.get_case(t,epoch)}\")\n",
        "    print(f\"{tt}: \\t {val_metric_keeper.get_case(tt,epoch)}\")\n",
        "  print(f\"Loss: \\t {val_metric_keeper.get('val_loss', epoch)}\")\n",
        "  print(f\"REAL LOSS: \\t {val_metric_keeper.get('val_real_loss', epoch)}\")\n",
        "  print(f\"ACC: \\t {val_metric_keeper.get('val_acc', epoch)}\")\n",
        "  print(f\"BEST ACC: \\t {best_val_acc}\")\n",
        "  print(f\"Positive ACC: \\t {val_metric_keeper.get('val_positive_acc', epoch)}\")\n",
        "  print(f\"Negative ACC: \\t {val_metric_keeper.get('val_negative_acc', epoch)}\")\n",
        "  print(f\"REAL ACC: \\t {val_metric_keeper.get('val_real_acc', epoch)}\")\n",
        "  print(f\"Positive REAL ACC: \\t {val_metric_keeper.get('val_positive_real_acc', epoch)}\")\n",
        "  print(f\"Negative REAL ACC: \\t {val_metric_keeper.get('val_negative_real_acc', epoch)}\")\n",
        "  print(f\"RECALL: \\t {val_metric_keeper.get('val_recall', epoch)}\")\n",
        "  print(f\"PRECISION: \\t {val_metric_keeper.get('val_precision', epoch)}\")\n",
        "  print(f\"F1_SCORE: \\t {val_metric_keeper.get('val_f_one_score', epoch)}\")\n",
        "  print('-'*100)\n",
        "  ####################################################\n",
        "  net.train()\n",
        "  ####################################################\n",
        "  state = ({'epoch' : epoch,'state_dict' : net.state_dict(),'optimizer' : optimizer.state_dict(),'best_val_acc' : best_val_acc})\n",
        "\n",
        "  fn = os.path.join(model_save_dir, get_checkpoint_fname())\n",
        "  best_fn = os.path.join(model_save_dir, f'best_val_{get_checkpoint_fname()}')\n",
        "  print(f'Saving model to: {fn}')\n",
        "  torch.save(state, fn)\n",
        "  # save state of the best validation    \n",
        "  if is_best:\n",
        "    print(f'Saving best val acc model to: {best_fn}')\n",
        "    torch.save(state, best_fn)\n",
        "  after = datetime.datetime.now()\n",
        "  print(f'total time passed for the epoch {epoch}: {(after-before_all).seconds} seconds...')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "************** SETUP PARAMETERS **************\n",
            "BATCH_SIZE:32\n",
            "TRANSFORM_RESIZE:224\n",
            "MAX_EPOCHS:50\n",
            "NOISE_RATIO:0\n",
            "LR:0.005\n",
            "LR_DECAY:0.1\n",
            "LR_DECAY_PER_EPOCH:10\n",
            "MOMENTUM:0.9\n",
            "WEIGHT_DECAY:0.0001\n",
            "LOG_ITERATION_FACTOR:100\n",
            "RESUME:0\n",
            "**********************************************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/archive/v0.6.0.zip\" to /root/.cache/torch/hub/v0.6.0.zip\n",
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67e1a4487c1243aba149f8947187be79",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=32342954.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training WITHOUT loading a network...\n",
            "********************TRAIN e:0********************\n",
            "Learning rate of epoch:0 is: 0.005\n",
            "Logging train results of iteration: 99 on epoch:0 with total itration: 99\n",
            "100/832(12.02%) finished. Estimated Remaining Time: 212.28 seconds.\n",
            "Logging train results of iteration: 199 on epoch:0 with total itration: 199\n",
            "200/832(24.04%) finished. Estimated Remaining Time: 180.12 seconds.\n",
            "Logging train results of iteration: 299 on epoch:0 with total itration: 299\n",
            "300/832(36.06%) finished. Estimated Remaining Time: 150.73 seconds.\n",
            "Logging train results of iteration: 399 on epoch:0 with total itration: 399\n",
            "400/832(48.08%) finished. Estimated Remaining Time: 122.04 seconds.\n",
            "Logging train results of iteration: 499 on epoch:0 with total itration: 499\n",
            "500/832(60.10%) finished. Estimated Remaining Time: 93.62 seconds.\n",
            "Logging train results of iteration: 599 on epoch:0 with total itration: 599\n",
            "600/832(72.12%) finished. Estimated Remaining Time: 64.96 seconds.\n",
            "Logging train results of iteration: 699 on epoch:0 with total itration: 699\n",
            "700/832(84.13%) finished. Estimated Remaining Time: 36.77 seconds.\n",
            "Logging train results of iteration: 799 on epoch:0 with total itration: 799\n",
            "800/832(96.15%) finished. Estimated Remaining Time: 8.92 seconds.\n",
            "TRAIN EPOCH END [0/50]\n",
            "train_enlarged_cardiomediastinum_acc: \t 0.5655290102389078\n",
            "train_enlarged_cardiomediastinum_real_acc: \t 0.5655290102389078\n",
            "train_cardiomegaly_acc: \t 0.5361720067453626\n",
            "train_cardiomegaly_real_acc: \t 0.5361720067453626\n",
            "train_lung_opacity_acc: \t 0.543901766004415\n",
            "train_lung_opacity_real_acc: \t 0.543901766004415\n",
            "train_lung_lesion_acc: \t 0.5626502732240438\n",
            "train_lung_lesion_real_acc: \t 0.5626502732240438\n",
            "train_edema_acc: \t 0.5378507295173961\n",
            "train_edema_real_acc: \t 0.5378507295173961\n",
            "train_consolidation_acc: \t 0.51463454206214\n",
            "train_consolidation_real_acc: \t 0.51463454206214\n",
            "train_pneumonia_acc: \t 0.532717086834734\n",
            "train_pneumonia_real_acc: \t 0.532717086834734\n",
            "train_atelectasis_acc: \t 0.552980324074074\n",
            "train_atelectasis_real_acc: \t 0.552980324074074\n",
            "train_pneumothorax_acc: \t 0.5995719178082192\n",
            "train_pneumothorax_real_acc: \t 0.5995719178082192\n",
            "train_pleural_effusion_acc: \t 0.5334467120181406\n",
            "train_pleural_effusion_real_acc: \t 0.5334467120181406\n",
            "train_pleural_other_acc: \t 0.5393837535014006\n",
            "train_pleural_other_real_acc: \t 0.5393837535014006\n",
            "train_fracture_acc: \t 0.5762149799885649\n",
            "train_fracture_real_acc: \t 0.5762149799885649\n",
            "train_support_devices_acc: \t 0.5576111111111111\n",
            "train_support_devices_real_acc: \t 0.5576111111111111\n",
            "Loss: \t 0.7164394818962767\n",
            "REAL LOSS: \t 0.7164394818962767\n",
            "ACC: \t 0.5113055889423077\n",
            "Positive ACC: \t 0.5528123630537464\n",
            "Negative ACC: \t 0.46772425131889034\n",
            "REAL ACC: \t 0.5113055889423077\n",
            "Positive REAL ACC: \t 0.5528123630537464\n",
            "Negative REAL ACC: \t 0.46772425131889034\n",
            "RECALL: \t 0.5113055889423077\n",
            "PRECISION: \t 0.5113055889423077\n",
            "F1_SCORE: \t 0.5113055889423077\n",
            "----------------------------------------------------------------------------------------------------\n",
            "********************VALIDATION e:0********************\n",
            "Logging VAL results of iteration: 99 on epoch:0 with total itration: 99\n",
            "100/104(96.15%) finished. Estimated Remaining Time: 0.44 seconds.\n",
            "VALIDATION EPOCH END [0/50]\n",
            "val_enlarged_cardiomediastinum_acc: \t 1.0\n",
            "val_enlarged_cardiomediastinum_real_acc: \t 1.0\n",
            "val_cardiomegaly_acc: \t 1.0\n",
            "val_cardiomegaly_real_acc: \t 1.0\n",
            "val_lung_opacity_acc: \t 1.0\n",
            "val_lung_opacity_real_acc: \t 1.0\n",
            "val_lung_lesion_acc: \t 1.0\n",
            "val_lung_lesion_real_acc: \t 1.0\n",
            "val_edema_acc: \t 1.0\n",
            "val_edema_real_acc: \t 1.0\n",
            "val_consolidation_acc: \t 1.0\n",
            "val_consolidation_real_acc: \t 1.0\n",
            "val_pneumonia_acc: \t 1.0\n",
            "val_pneumonia_real_acc: \t 1.0\n",
            "val_atelectasis_acc: \t 1.0\n",
            "val_atelectasis_real_acc: \t 1.0\n",
            "val_pneumothorax_acc: \t 1.0\n",
            "val_pneumothorax_real_acc: \t 1.0\n",
            "val_pleural_effusion_acc: \t 1.0\n",
            "val_pleural_effusion_real_acc: \t 1.0\n",
            "val_pleural_other_acc: \t 1.0\n",
            "val_pleural_other_real_acc: \t 1.0\n",
            "val_fracture_acc: \t 1.0\n",
            "val_fracture_real_acc: \t 1.0\n",
            "val_support_devices_acc: \t 1.0\n",
            "val_support_devices_real_acc: \t 1.0\n",
            "Loss: \t 0.7115007220552518\n",
            "REAL LOSS: \t 0.7115007220552518\n",
            "ACC: \t 0.5\n",
            "BEST ACC: \t 0.5\n",
            "Positive ACC: \t 1.0\n",
            "Negative ACC: \t 0.0\n",
            "REAL ACC: \t 0.5\n",
            "Positive REAL ACC: \t 1.0\n",
            "Negative REAL ACC: \t 0.0\n",
            "RECALL: \t 0.5\n",
            "PRECISION: \t 0.5\n",
            "F1_SCORE: \t 0.5\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Saving model to: ./models/torch_baseline/lr_[5e-3]_wd_[5e-3]_b_32_checkpoint.pth.tar\n",
            "Saving best val acc model to: ./models/torch_baseline/best_val_lr_[5e-3]_wd_[5e-3]_b_32_checkpoint.pth.tar\n",
            "total time passed for the epoch 0: 275 seconds...\n",
            "********************TRAIN e:1********************\n",
            "Learning rate of epoch:1 is: 0.005\n",
            "Logging train results of iteration: 67 on epoch:1 with total itration: 899\n",
            "68/832(8.17%) finished. Estimated Remaining Time: 224.71 seconds.\n",
            "Logging train results of iteration: 167 on epoch:1 with total itration: 999\n",
            "168/832(20.19%) finished. Estimated Remaining Time: 185.76 seconds.\n",
            "Logging train results of iteration: 267 on epoch:1 with total itration: 1099\n",
            "268/832(32.21%) finished. Estimated Remaining Time: 157.84 seconds.\n",
            "Logging train results of iteration: 367 on epoch:1 with total itration: 1199\n",
            "368/832(44.23%) finished. Estimated Remaining Time: 128.61 seconds.\n",
            "Logging train results of iteration: 467 on epoch:1 with total itration: 1299\n",
            "468/832(56.25%) finished. Estimated Remaining Time: 100.33 seconds.\n",
            "Logging train results of iteration: 567 on epoch:1 with total itration: 1399\n",
            "568/832(68.27%) finished. Estimated Remaining Time: 72.97 seconds.\n",
            "Logging train results of iteration: 667 on epoch:1 with total itration: 1499\n",
            "668/832(80.29%) finished. Estimated Remaining Time: 45.17 seconds.\n",
            "Logging train results of iteration: 767 on epoch:1 with total itration: 1599\n",
            "768/832(92.31%) finished. Estimated Remaining Time: 17.58 seconds.\n",
            "TRAIN EPOCH END [1/50]\n",
            "train_enlarged_cardiomediastinum_acc: \t 0.5535634118967453\n",
            "train_enlarged_cardiomediastinum_real_acc: \t 0.5535634118967453\n",
            "train_cardiomegaly_acc: \t 0.5649653979238753\n",
            "train_cardiomegaly_real_acc: \t 0.5649653979238753\n",
            "train_lung_opacity_acc: \t 0.5358527131782946\n",
            "train_lung_opacity_real_acc: \t 0.5358527131782946\n",
            "train_lung_lesion_acc: \t 0.5642536475869808\n",
            "train_lung_lesion_real_acc: \t 0.5642536475869808\n",
            "train_edema_acc: \t 0.5343218012081274\n",
            "train_edema_real_acc: \t 0.5343218012081274\n",
            "train_consolidation_acc: \t 0.5353458049886621\n",
            "train_consolidation_real_acc: \t 0.5353458049886621\n",
            "train_pneumonia_acc: \t 0.5454102122776822\n",
            "train_pneumonia_real_acc: \t 0.5454102122776822\n",
            "train_atelectasis_acc: \t 0.5243067345783813\n",
            "train_atelectasis_real_acc: \t 0.5243067345783813\n",
            "train_pneumothorax_acc: \t 0.5835304054054054\n",
            "train_pneumothorax_real_acc: \t 0.5835304054054054\n",
            "train_pleural_effusion_acc: \t 0.5431111111111111\n",
            "train_pleural_effusion_real_acc: \t 0.5431111111111111\n",
            "train_pleural_other_acc: \t 0.5413288288288288\n",
            "train_pleural_other_real_acc: \t 0.5413288288288288\n",
            "train_fracture_acc: \t 0.5578703703703703\n",
            "train_fracture_real_acc: \t 0.5578703703703703\n",
            "train_support_devices_acc: \t 0.5546280991735536\n",
            "train_support_devices_real_acc: \t 0.5546280991735536\n",
            "Loss: \t 0.6944130869725575\n",
            "REAL LOSS: \t 0.6944130869725575\n",
            "ACC: \t 0.5081129807692307\n",
            "Positive ACC: \t 0.5466054382408528\n",
            "Negative ACC: \t 0.46406859357144287\n",
            "REAL ACC: \t 0.5081129807692307\n",
            "Positive REAL ACC: \t 0.5466054382408528\n",
            "Negative REAL ACC: \t 0.46406859357144287\n",
            "RECALL: \t 0.5081129807692307\n",
            "PRECISION: \t 0.5081129807692307\n",
            "F1_SCORE: \t 0.5081129807692307\n",
            "----------------------------------------------------------------------------------------------------\n",
            "********************VALIDATION e:1********************\n",
            "Logging VAL results of iteration: 95 on epoch:1 with total itration: 199\n",
            "96/104(92.31%) finished. Estimated Remaining Time: 0.92 seconds.\n",
            "VALIDATION EPOCH END [1/50]\n",
            "val_enlarged_cardiomediastinum_acc: \t 1.0\n",
            "val_enlarged_cardiomediastinum_real_acc: \t 1.0\n",
            "val_cardiomegaly_acc: \t 1.0\n",
            "val_cardiomegaly_real_acc: \t 1.0\n",
            "val_lung_opacity_acc: \t 1.0\n",
            "val_lung_opacity_real_acc: \t 1.0\n",
            "val_lung_lesion_acc: \t 1.0\n",
            "val_lung_lesion_real_acc: \t 1.0\n",
            "val_edema_acc: \t 1.0\n",
            "val_edema_real_acc: \t 1.0\n",
            "val_consolidation_acc: \t 0.9939024390243902\n",
            "val_consolidation_real_acc: \t 0.9939024390243902\n",
            "val_pneumonia_acc: \t 1.0\n",
            "val_pneumonia_real_acc: \t 1.0\n",
            "val_atelectasis_acc: \t 1.0\n",
            "val_atelectasis_real_acc: \t 1.0\n",
            "val_pneumothorax_acc: \t 1.0\n",
            "val_pneumothorax_real_acc: \t 1.0\n",
            "val_pleural_effusion_acc: \t 1.0\n",
            "val_pleural_effusion_real_acc: \t 1.0\n",
            "val_pleural_other_acc: \t 0.9863013698630136\n",
            "val_pleural_other_real_acc: \t 0.9863013698630136\n",
            "val_fracture_acc: \t 1.0\n",
            "val_fracture_real_acc: \t 1.0\n",
            "val_support_devices_acc: \t 1.0\n",
            "val_support_devices_real_acc: \t 1.0\n",
            "Loss: \t 0.693226987352738\n",
            "REAL LOSS: \t 0.693226987352738\n",
            "ACC: \t 0.49939903846153844\n",
            "BEST ACC: \t 0.49939903846153844\n",
            "Positive ACC: \t 0.9986593934911242\n",
            "Negative ACC: \t 0.0\n",
            "REAL ACC: \t 0.49939903846153844\n",
            "Positive REAL ACC: \t 0.9986593934911242\n",
            "Negative REAL ACC: \t 0.0\n",
            "RECALL: \t 0.49939903846153844\n",
            "PRECISION: \t 0.49939903846153844\n",
            "F1_SCORE: \t 0.49939903846153844\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Saving model to: ./models/torch_baseline/lr_[5e-3]_wd_[5e-3]_b_32_checkpoint.pth.tar\n",
            "Saving best val acc model to: ./models/torch_baseline/best_val_lr_[5e-3]_wd_[5e-3]_b_32_checkpoint.pth.tar\n",
            "total time passed for the epoch 1: 269 seconds...\n",
            "********************TRAIN e:2********************\n",
            "Learning rate of epoch:2 is: 0.005\n",
            "Logging train results of iteration: 35 on epoch:2 with total itration: 1699\n",
            "36/832(4.33%) finished. Estimated Remaining Time: 265.33 seconds.\n",
            "Logging train results of iteration: 135 on epoch:2 with total itration: 1799\n",
            "136/832(16.35%) finished. Estimated Remaining Time: 204.71 seconds.\n",
            "Logging train results of iteration: 235 on epoch:2 with total itration: 1899\n",
            "236/832(28.37%) finished. Estimated Remaining Time: 169.20 seconds.\n",
            "Logging train results of iteration: 335 on epoch:2 with total itration: 1999\n",
            "336/832(40.38%) finished. Estimated Remaining Time: 140.24 seconds.\n",
            "Logging train results of iteration: 435 on epoch:2 with total itration: 2099\n",
            "436/832(52.40%) finished. Estimated Remaining Time: 109.90 seconds.\n",
            "Logging train results of iteration: 535 on epoch:2 with total itration: 2199\n",
            "536/832(64.42%) finished. Estimated Remaining Time: 81.73 seconds.\n",
            "Logging train results of iteration: 635 on epoch:2 with total itration: 2299\n",
            "636/832(76.44%) finished. Estimated Remaining Time: 54.24 seconds.\n",
            "Logging train results of iteration: 735 on epoch:2 with total itration: 2399\n",
            "736/832(88.46%) finished. Estimated Remaining Time: 26.48 seconds.\n",
            "TRAIN EPOCH END [2/50]\n",
            "train_enlarged_cardiomediastinum_acc: \t 0.6676944444444445\n",
            "train_enlarged_cardiomediastinum_real_acc: \t 0.6676944444444445\n",
            "train_cardiomegaly_acc: \t 0.6054257095158597\n",
            "train_cardiomegaly_real_acc: \t 0.6054257095158597\n",
            "train_lung_opacity_acc: \t 0.6716340933767644\n",
            "train_lung_opacity_real_acc: \t 0.6716340933767644\n",
            "train_lung_lesion_acc: \t 0.6626297577854672\n",
            "train_lung_lesion_real_acc: \t 0.6626297577854672\n",
            "train_edema_acc: \t 0.5945945945945946\n",
            "train_edema_real_acc: \t 0.5945945945945946\n",
            "train_consolidation_acc: \t 0.597741046831956\n",
            "train_consolidation_real_acc: \t 0.597741046831956\n",
            "train_pneumonia_acc: \t 0.6466996699669967\n",
            "train_pneumonia_real_acc: \t 0.6466996699669967\n",
            "train_atelectasis_acc: \t 0.6623785020011435\n",
            "train_atelectasis_real_acc: \t 0.6623785020011435\n",
            "train_pneumothorax_acc: \t 0.7288288288288289\n",
            "train_pneumothorax_real_acc: \t 0.7288288288288289\n",
            "train_pleural_effusion_acc: \t 0.6143437862950057\n",
            "train_pleural_effusion_real_acc: \t 0.6143437862950057\n",
            "train_pleural_other_acc: \t 0.6227908277404921\n",
            "train_pleural_other_real_acc: \t 0.6227908277404921\n",
            "train_fracture_acc: \t 0.6976593344613649\n",
            "train_fracture_real_acc: \t 0.6976593344613649\n",
            "train_support_devices_acc: \t 0.675406162464986\n",
            "train_support_devices_real_acc: \t 0.675406162464986\n",
            "Loss: \t 0.6893988193657535\n",
            "REAL LOSS: \t 0.6893988193657535\n",
            "ACC: \t 0.5289588341346154\n",
            "Positive ACC: \t 0.6490628702233097\n",
            "Negative ACC: \t 0.40620505447501815\n",
            "REAL ACC: \t 0.5289588341346154\n",
            "Positive REAL ACC: \t 0.6490628702233097\n",
            "Negative REAL ACC: \t 0.40620505447501815\n",
            "RECALL: \t 0.5289588341346154\n",
            "PRECISION: \t 0.5289588341346154\n",
            "F1_SCORE: \t 0.5289588341346154\n",
            "----------------------------------------------------------------------------------------------------\n",
            "********************VALIDATION e:2********************\n",
            "Logging VAL results of iteration: 91 on epoch:2 with total itration: 299\n",
            "92/104(88.46%) finished. Estimated Remaining Time: 1.43 seconds.\n",
            "VALIDATION EPOCH END [2/50]\n",
            "val_enlarged_cardiomediastinum_acc: \t 0.0\n",
            "val_enlarged_cardiomediastinum_real_acc: \t 0.0\n",
            "val_cardiomegaly_acc: \t 0.0\n",
            "val_cardiomegaly_real_acc: \t 0.0\n",
            "val_lung_opacity_acc: \t 0.0\n",
            "val_lung_opacity_real_acc: \t 0.0\n",
            "val_lung_lesion_acc: \t 0.0\n",
            "val_lung_lesion_real_acc: \t 0.0\n",
            "val_edema_acc: \t 0.0\n",
            "val_edema_real_acc: \t 0.0\n",
            "val_consolidation_acc: \t 0.0\n",
            "val_consolidation_real_acc: \t 0.0\n",
            "val_pneumonia_acc: \t 0.0\n",
            "val_pneumonia_real_acc: \t 0.0\n",
            "val_atelectasis_acc: \t 0.0\n",
            "val_atelectasis_real_acc: \t 0.0\n",
            "val_pneumothorax_acc: \t 0.0\n",
            "val_pneumothorax_real_acc: \t 0.0\n",
            "val_pleural_effusion_acc: \t 0.0\n",
            "val_pleural_effusion_real_acc: \t 0.0\n",
            "val_pleural_other_acc: \t 0.0\n",
            "val_pleural_other_real_acc: \t 0.0\n",
            "val_fracture_acc: \t 0.0\n",
            "val_fracture_real_acc: \t 0.0\n",
            "val_support_devices_acc: \t 0.0\n",
            "val_support_devices_real_acc: \t 0.0\n",
            "Loss: \t 178.55374769064096\n",
            "REAL LOSS: \t 178.55374769064096\n",
            "ACC: \t 0.5\n",
            "BEST ACC: \t 0.5\n",
            "Positive ACC: \t 0.0\n",
            "Negative ACC: \t 1.0\n",
            "REAL ACC: \t 0.5\n",
            "Positive REAL ACC: \t 0.0\n",
            "Negative REAL ACC: \t 1.0\n",
            "RECALL: \t 0.5\n",
            "PRECISION: \t 0.5\n",
            "F1_SCORE: \t 0.5\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Saving model to: ./models/torch_baseline/lr_[5e-3]_wd_[5e-3]_b_32_checkpoint.pth.tar\n",
            "Saving best val acc model to: ./models/torch_baseline/best_val_lr_[5e-3]_wd_[5e-3]_b_32_checkpoint.pth.tar\n",
            "total time passed for the epoch 2: 269 seconds...\n",
            "********************TRAIN e:3********************\n",
            "Learning rate of epoch:3 is: 0.005\n",
            "Logging train results of iteration: 3 on epoch:3 with total itration: 2499\n",
            "4/832(0.48%) finished. Estimated Remaining Time: 1035.00 seconds.\n",
            "Logging train results of iteration: 103 on epoch:3 with total itration: 2599\n",
            "104/832(12.50%) finished. Estimated Remaining Time: 231.00 seconds.\n",
            "Logging train results of iteration: 203 on epoch:3 with total itration: 2699\n",
            "204/832(24.52%) finished. Estimated Remaining Time: 184.71 seconds.\n",
            "Logging train results of iteration: 303 on epoch:3 with total itration: 2799\n",
            "304/832(36.54%) finished. Estimated Remaining Time: 151.11 seconds.\n",
            "Logging train results of iteration: 403 on epoch:3 with total itration: 2899\n",
            "404/832(48.56%) finished. Estimated Remaining Time: 120.77 seconds.\n",
            "Logging train results of iteration: 503 on epoch:3 with total itration: 2999\n",
            "504/832(60.58%) finished. Estimated Remaining Time: 91.76 seconds.\n",
            "Logging train results of iteration: 603 on epoch:3 with total itration: 3099\n",
            "604/832(72.60%) finished. Estimated Remaining Time: 63.42 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AWrqmi14qF0",
        "colab_type": "text"
      },
      "source": [
        "update 37\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9we1PCeFS23",
        "colab_type": "text"
      },
      "source": [
        "# pytorch playground"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz3OJdlEtqMo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjTVOB-otrnQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "fe1a6e88-d5ba-40b3-ea2d-5f1c42041b88"
      },
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfhCv_uDpSRb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "e4bbacc2-5f0c-4efc-80f4-b3600ad91db2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# functions to show an image\n",
        "\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-175-65b629ef6bb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# get some random training images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdataiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# show images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhazAYJCt9Q1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tPcL57euBLE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OTSP0wAuF5-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "8afd426c-81d0-483e-bb1e-1a7dded5b252"
      },
      "source": [
        "num_classes  =10\n",
        "train_preds  \t  = torch.zeros(len(trainset), num_classes) - 1.\n",
        "num_hist = 10\n",
        "train_preds_hist  = torch.zeros(len(trainset), num_hist, num_classes)\n",
        "pl_ratio = 0.; nl_ratio = 1.-pl_ratio\n",
        "train_losses      = torch.zeros(len(trainset)) - 1.\n",
        "CC = 300\n",
        "for epoch in range(1):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels, index = data\n",
        "        #index = torch.ones(32) + i*32\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # get the predictions\n",
        "        _, pred = torch.max(outputs.data, -1)      \n",
        "        acc = float((pred==labels.data).sum())\n",
        "        train_acc += acc\n",
        "        # #Â # - # #Â # - # #Â #\n",
        "        train_preds[index.cpu()] = F.softmax(outputs, -1).cpu().data\n",
        "        train_losses[i] = loss.data\n",
        "        # #Â # - # #Â # - # #Â #\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # #Â # - # #Â # - # #Â #\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % CC == CC - 1:    # print every CC mini-batches)\n",
        "            print('[%d, %5d] loss: %.3f acc: %.3f' % (epoch + 1, i + 1, running_loss / CC, train_acc/(CC)))\n",
        "            running_loss = 0.0\n",
        "            train_acc = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-148-b1b51a20f006>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# get the inputs; data is a list of [inputs, labels]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;31m#index = torch.ones(32) + i*32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# zero the parameter gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Efga1r_T1Sg_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GVHe_4pyLbC",
        "colab_type": "text"
      },
      "source": [
        "# New Playground"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlkV27N31HhI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def channel_check(sample):\n",
        "  for i in range(256):\n",
        "    c = (sample[i][:,0] == sample[i][:,1]).all() and (sample[i][:,1] == sample[i][:,2]).all()\n",
        "    if not c:\n",
        "      return False\n",
        "  \n",
        "  return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIEuAjpGzRyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "length = len(np_train_dataset)\n",
        "print_mod = 10\n",
        "before = datetime.datetime.now()\n",
        "for i in range(length):\n",
        "  c = channel_check(np_train_dataset[i])\n",
        "  if (i+1)%print_mod == 0:\n",
        "    print('*'*20+f'{i}'+'*'*20)\n",
        "    print_mod *= 2\n",
        "    print_remaining_time(before, i+1, length)\n",
        "\n",
        "  if not c:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6AKiUm016a-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaMvAFWJ3PeL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "length = len(np_val_dataset)\n",
        "print_mod = 10\n",
        "before = datetime.datetime.now()\n",
        "for i in range(length):\n",
        "  c = channel_check(np_val_dataset[i])\n",
        "  if (i+1)%print_mod == 0:\n",
        "    print('*'*20+f'{i}'+'*'*20)\n",
        "    print_mod *= 2\n",
        "    print_remaining_time(before, i+1, length)\n",
        "    \n",
        "  if not c:\n",
        "\n",
        "    break\n",
        "print(c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDHeiOQ15ro9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "18481892-0b44-40ae-af65-59cfe6fa9f19"
      },
      "source": [
        "length = len(np_test_dataset)\n",
        "print_mod = 10\n",
        "before = datetime.datetime.now()\n",
        "for i in range(length):\n",
        "  c = channel_check(np_test_dataset[i])\n",
        "  if (i+1)%print_mod == 0:\n",
        "    print('*'*20+f'{i}'+'*'*20)\n",
        "    print_mod *= 2\n",
        "    print_remaining_time(before, i+1, length)\n",
        "\n",
        "  if not c:\n",
        "    break\n",
        "print(c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "********************9********************\n",
            "10/3356(0.30%) finished. Estimated Remaining Time: 0.00 seconds.\n",
            "********************19********************\n",
            "20/3356(0.60%) finished. Estimated Remaining Time: 0.00 seconds.\n",
            "********************39********************\n",
            "40/3356(1.19%) finished. Estimated Remaining Time: 0.00 seconds.\n",
            "********************79********************\n",
            "80/3356(2.38%) finished. Estimated Remaining Time: 0.00 seconds.\n",
            "********************159********************\n",
            "160/3356(4.77%) finished. Estimated Remaining Time: 0.00 seconds.\n",
            "********************319********************\n",
            "320/3356(9.54%) finished. Estimated Remaining Time: 0.00 seconds.\n",
            "********************639********************\n",
            "640/3356(19.07%) finished. Estimated Remaining Time: 4.24 seconds.\n",
            "********************1279********************\n",
            "1280/3356(38.14%) finished. Estimated Remaining Time: 3.24 seconds.\n",
            "********************2559********************\n",
            "2560/3356(76.28%) finished. Estimated Remaining Time: 1.55 seconds.\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLKYB97Q5x-c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "fa148715-b78e-4656-b07f-196a920472cf"
      },
      "source": [
        "a = np_test_dataset[0] - 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3883884636f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp_test_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'np_test_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzG82NxnP2tr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}